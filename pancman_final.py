# -*- coding: utf-8 -*-
"""Pancman_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/179gw7R1PxCx-T6hdKG-6xUDhMgC526Js
"""

# ============ Q-learning Pacman ============

import random
import numpy as np

# ============ 1. Game Environment Class ============
class SimplePacman:
    """Pacman Game Environment - Designed for Q-learning"""

    def __init__(self, grid_size=10):
        self.GRID_SIZE = grid_size
        self.reset()

    def reset(self):
        """Reset game state"""
        self.pacman = [self.GRID_SIZE // 2, self.GRID_SIZE // 2]  # Center
        self.ghost = [1, 1]  # Top-left corner
        self.pellets = []
        self._generate_pellets(8)  # 8 pellets
        self.score = 0
        self.steps = 0
        self.game_over = False
        self.win = False
        return self.get_state()

    def _generate_pellets(self, count):
        """Generate pellets"""
        all_positions = [(x, y) for x in range(self.GRID_SIZE)
                        for y in range(self.GRID_SIZE)]
        all_positions.remove(tuple(self.pacman))
        all_positions.remove(tuple(self.ghost))
        self.pellets = random.sample(all_positions, min(count, len(all_positions)))

    def get_state(self):
        """Simple state representation"""
        px, py = self.pacman
        gx, gy = self.ghost

        # Ghost direction
        if gx > px:
            ghost_dir = 0  # Right
        elif gx < px:
            ghost_dir = 1  # Left
        elif gy > py:
            ghost_dir = 2  # Down
        elif gy < py:
            ghost_dir = 3  # Up
        else:
            ghost_dir = 4  # Same position

        # Pellet direction
        if self.pellets:
            closest = min(self.pellets, key=lambda p: abs(p[0]-px) + abs(p[1]-py))
            if closest[0] > px:
                pellet_dir = 0  # Right
            elif closest[0] < px:
                pellet_dir = 1  # Left
            elif closest[1] > py:
                pellet_dir = 2  # Down
            else:
                pellet_dir = 3  # Up
        else:
            pellet_dir = 4  # No pellets

        return (ghost_dir, pellet_dir)

    def move_ghost(self):
        """Simple ghost movement"""
        if random.random() < 0.5:  # Smart move
            dx = self.pacman[0] - self.ghost[0]
            dy = self.pacman[1] - self.ghost[1]

            # Move in primary direction
            if abs(dx) > abs(dy):
                if dx > 0 and self.ghost[0] < self.GRID_SIZE - 1:
                    self.ghost[0] += 1
                elif dx < 0 and self.ghost[0] > 0:
                    self.ghost[0] -= 1
            else:
                if dy > 0 and self.ghost[1] < self.GRID_SIZE - 1:
                    self.ghost[1] += 1
                elif dy < 0 and self.ghost[1] > 0:
                    self.ghost[1] -= 1
        else:  # Random move
            directions = []
            if self.ghost[0] < self.GRID_SIZE - 1: directions.append((1, 0))
            if self.ghost[0] > 0: directions.append((-1, 0))
            if self.ghost[1] < self.GRID_SIZE - 1: directions.append((0, 1))
            if self.ghost[1] > 0: directions.append((0, -1))

            if directions:
                dx, dy = random.choice(directions)
                self.ghost[0] += dx
                self.ghost[1] += dy

    def step(self, action):
        """Execute action - prevent boundary movement"""
        if self.game_over:
            return self.get_state(), 0, True

        self.steps += 1
        old_score = self.score

        # Get current position
        px, py = self.pacman

        # Check if move is valid
        valid_move = True
        if (py == 0 and action == 0) or \
          (py == self.GRID_SIZE - 1 and action == 1) or \
          (px == 0 and action == 2) or \
          (px == self.GRID_SIZE - 1 and action == 3):
          valid_move = False

        if not valid_move:
            # Invalid move: give penalty, position unchanged
            reward = -0.3
            next_state = self.get_state()
            return next_state, reward, self.game_over

        # Valid move: execute move
        if action == 0:  # Up
            self.pacman[1] -= 1
        elif action == 1:  # Down
            self.pacman[1] += 1
        elif action == 2:  # Left
            self.pacman[0] -= 1
        elif action == 3:  # Right
            self.pacman[0] += 1

        # Normal game logic
        if tuple(self.pacman) in self.pellets:
            self.pellets.remove(tuple(self.pacman))
            self.score += 10

        if self.pacman == self.ghost:
            self.score -= 50
            self.game_over = True
            reward = -5
        else:
            if (self.score - old_score) > 0:
                reward = 20
            else:
                reward = 0.1

        if not self.game_over:
            self.move_ghost()
            if self.pacman == self.ghost:
                self.score -= 50
                self.game_over = True
                reward = -5

        if not self.game_over and len(self.pellets) == 0:
            self.score += 100
            self.game_over = True
            self.win = True
            reward = 30

        next_state = self.get_state()

        if self.steps >= 100:
            self.game_over = True

        return next_state, reward, self.game_over

# ============ 2. Q-learning Agent Class ============
class QLearningAgent:

    def __init__(self, state_size=2, action_size=4,
                 learning_rate=0.1, discount_factor=0.9,
                 exploration_rate=0.2, exploration_decay=0.995,
                 exploration_min=0.01):
        self.alpha = learning_rate
        self.gamma = discount_factor
        self.epsilon = exploration_rate
        self.epsilon_decay = exploration_decay
        self.epsilon_min = exploration_min
        self.action_size = action_size
        self.q_table = {}
        self.training_history = {'rewards': [], 'epsilons': [], 'episodes': 0}

    def get_state_key(self, state):
        return str(state)

    def choose_action(self, state):
        state_key = self.get_state_key(state)
        if state_key not in self.q_table:
            self.q_table[state_key] = [0.0] * self.action_size

        if np.random.random() < self.epsilon:
            return np.random.randint(self.action_size)  # Explore
        else:
            q_values = self.q_table[state_key]
            max_q = max(q_values)
            best_actions = [i for i, q in enumerate(q_values) if q == max_q]
            return np.random.choice(best_actions)  # Exploit

    def learn(self, state, action, reward, next_state, done):
        state_key = self.get_state_key(state)
        next_key = self.get_state_key(next_state)

        if state_key not in self.q_table:
            self.q_table[state_key] = [0.0] * self.action_size
        if next_key not in self.q_table:
            self.q_table[next_key] = [0.0] * self.action_size

        current_q = self.q_table[state_key][action]
        if done:
            target = reward
        else:
            target = reward + self.gamma * max(self.q_table[next_key])

        new_q = current_q + self.alpha * (target - current_q)
        self.q_table[state_key][action] = new_q
        return new_q

    def update_after_episode(self, episode_reward):
        self.training_history['rewards'].append(episode_reward)
        self.training_history['epsilons'].append(self.epsilon)
        self.training_history['episodes'] += 1
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def save(self, filename='trained_agent.pkl'):
        """Save agent"""
        import pickle
        with open(filename, 'wb') as f:
            data = {
                'q_table': self.q_table,
                'params': {
                    'alpha': self.alpha,
                    'gamma': self.gamma,
                    'epsilon': self.epsilon
                },
                'history': self.training_history
            }
            pickle.dump(data, f)
        print(f"Agent saved to {filename}")

    def load(self, filename='trained_agent.pkl'):
        """Load agent"""
        import pickle
        try:
            with open(filename, 'rb') as f:
                data = pickle.load(f)
                self.q_table = data['q_table']
                params = data.get('params', {})
                self.alpha = params.get('alpha', self.alpha)
                self.gamma = params.get('gamma', self.gamma)
                self.epsilon = params.get('epsilon', self.epsilon)
                self.training_history = data.get('history', self.training_history)
            print(f"Agent loaded from {filename}")
            return True
        except FileNotFoundError:
            print(f"File {filename} not found")
            return False

# ============ 3. Training Function ============
def train_q_learning(episodes=300):
    """Train Q-learning agent"""
    env = SimplePacman(grid_size=10)
    agent = QLearningAgent(state_size=2)

    print("=" * 60)
    print("Starting Q-learning Training")
    print("=" * 60)
    print("Training Parameters: α=0.1, γ=0.9, ε₀=0.2, decay=0.995")
    print("-" * 60)

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False
        steps = 0

        while not done:
            action = agent.choose_action(state)
            next_state, reward, done = env.step(action)
            agent.learn(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward
            steps += 1

        agent.update_after_episode(total_reward)

        if (episode + 1) % 50 == 0 or episode == 0:
            recent = agent.training_history['rewards'][-50:]
            avg = np.mean(recent) if recent else 0
            print(f"Episode {episode+1:4d} | Reward: {total_reward:6.1f} | "
                  f"Avg(last 50): {avg:6.1f} | ε: {agent.epsilon:.3f} | "
                  f"States: {len(agent.q_table)}")

    print("-" * 60)
    print("Training Complete!")
    print(f"Final Average Reward(last 50): {np.mean(agent.training_history['rewards'][-50:]):.1f}")
    print(f"Total Learned States: {len(agent.q_table)}")

    # Save training results
    agent.save('trained_agent.pkl')
    return agent

# ============ 4. AI Demo Mode ============
def ai_demo_game(episodes=2, step_delay=0.3):
    """Watch AI play the game"""
    import time

    print("=" * 60)
    print("AI Demo Mode - Watch Trained AI Play")
    print("=" * 60)

    # Load trained AI
    agent = QLearningAgent(state_size=2)
    if not agent.load('trained_agent.pkl'):
        print("No trained AI model found")
        print("Please run Mode 1 to train AI, or ensure 'trained_agent.pkl' exists")
        return

    print(f"AI Loaded | States: {len(agent.q_table)} | Training Episodes: {agent.training_history['episodes']}")
    print(f"Will demo {episodes} games, step delay {step_delay} seconds")
    print("-" * 60)

    # Turn off exploration (pure exploitation)
    original_epsilon = agent.epsilon
    agent.epsilon = 0

    for game_num in range(1, episodes + 1):
        env = SimplePacman(grid_size=10)
        state = env.reset()
        done = False
        steps = 0

        print(f"\nGame {game_num} Started")
        print(f"Initial State: {state}")
        print("-" * 40)

        while not done and steps < 100:  # Max 100 steps
            steps += 1

            # 1. Display current game state
            print(f"\nStep: {steps:2d} | Score: {env.score:3d} | Pellets Left: {len(env.pellets)}")

            # Display grid BEFORE action (current state)
            print("   " + " ".join(str(i) for i in range(env.GRID_SIZE)))
            print("  +" + "-" * (env.GRID_SIZE * 2 - 1) + "+")

            for y in range(env.GRID_SIZE):
                row = f"{y} |"
                for x in range(env.GRID_SIZE):
                    if [x, y] == env.pacman:
                        row += "P "  # Pacman
                    elif [x, y] == env.ghost:
                        row += "G "  # Ghost
                    elif (x, y) in env.pellets:
                        row += ". "  # Pellet
                    else:
                        row += "  "
                row += "|"
                print(row)

            print("  +" + "-" * (env.GRID_SIZE * 2 - 1) + "+")

            # 2. Explain current state (human readable)
            ghost_dirs = ["right", "left", "below", "above"]
            pellet_dirs = ["right", "left", "below", "above", "no pellets"]

            print(f"AI Sees: Ghost is {ghost_dirs[state[0]]}, Pellet is {pellet_dirs[state[1]]}")

            # 3. AI chooses action
            action = agent.choose_action(state)
            action_names = ["Up", "Down", "Left", "Right"]
            print(f"AI Decision: Move {action_names[action]}")

            # 4. Execute action
            next_state, reward, done = env.step(action)

            # 5. Show result
            if reward > 0:
                print(f"Result: Gained reward +{reward:.1f}")
            elif reward < 0:
                print(f"Result: Received penalty {reward:.1f}")
            else:
                print(f"Result: Movement reward +{reward:.1f}")

            # 6. Update state
            state = next_state

            # 7. If game over, show final state
            if done:
                print("\n" + "="*40)
                if env.win:
                    print("VICTORY - All pellets eaten!")
                else:
                    print("GAME OVER - Pacman caught by ghost!")

                # Show final grid state
                print("\nFinal game state:")
                print("   " + " ".join(str(i) for i in range(env.GRID_SIZE)))
                print("  +" + "-" * (env.GRID_SIZE * 2 - 1) + "+")

                for y in range(env.GRID_SIZE):
                    row = f"{y} |"
                    for x in range(env.GRID_SIZE):
                        # Check if collision position (failure case)
                        if not env.win and [x, y] == env.pacman and [x, y] == env.ghost:
                            row += "X "  # Collision position
                        elif [x, y] == env.pacman:
                            row += "P "  # Pacman
                        elif [x, y] == env.ghost:
                            row += "G "  # Ghost
                        elif (x, y) in env.pellets:
                            row += ". "  # Remaining pellets (should be none if victory)
                        else:
                            row += "  "
                    row += "|"
                    print(row)

                print("  +" + "-" * (env.GRID_SIZE * 2 - 1) + "+")

                if env.win:
                    print(f"Perfect! All pellets collected!")
                print(f"Final Score: {env.score} | Total Steps: {steps}")
                print("="*40)
                break

            # Delay for viewing
            time.sleep(step_delay)

        if steps >= 100:
            print(f"\nGame Over: Reached maximum step limit")
            print(f"Final Score: {env.score} | Pellets Left: {len(env.pellets)}")

        print("=" * 40)
        time.sleep(1)  # Pause between games

    # Restore epsilon
    agent.epsilon = original_epsilon
    print("\n" + "=" * 60)
    print("Demo Complete! AI returned to exploration mode")
    print("=" * 60)

# ============ 5. Manual Game Mode ============
def play_game():
    """Player-controlled game"""
    env = SimplePacman(grid_size=10)

    print("Pacman Game - Manual Control Mode")
    print("=" * 50)
    print("Controls: W=Up, S=Down, A=Left, D=Right")
    print("Other: R=Reset, Q=Quit")
    print("Goal: Control P to eat all ·, avoid G")
    print("=" * 50)

    while True:
        print(f"\n{'='*40}")
        print(f"Score: {env.score:4d} | Steps: {env.steps:3d} | Pellets Left: {len(env.pellets)}")
        print(f"Pacman: ({env.pacman[0]},{env.pacman[1]}) | Ghost: ({env.ghost[0]},{env.ghost[1]})")

        # Display grid
        print("\n   " + " ".join(str(i) for i in range(env.GRID_SIZE)))
        print("  +" + "-" * (env.GRID_SIZE * 2 - 1) + "+")

        for y in range(env.GRID_SIZE):
            row = f"{y} |"
            for x in range(env.GRID_SIZE):
                if [x, y] == env.pacman:
                    row += "P "
                elif [x, y] == env.ghost:
                    row += "G "
                elif (x, y) in env.pellets:
                    row += ". "
                else:
                    row += "  "
            row += "|"
            print(row)

        print("  +" + "-" * (env.GRID_SIZE * 2 - 1) + "+")

        if env.game_over:
            print("\n" + ("Victory!" if env.win else "Game Over!"))
            break

        cmd = input("\nDirection(w-up/s-down/a-left/d-right) or q-quit: ").lower()
        if cmd == 'q':
            print("Game Quit")
            break

        action_map = {'w': 0, 's': 1, 'a': 2, 'd': 3}
        if cmd in action_map:
            state, reward, done = env.step(action_map[cmd])
            if reward > 0:
                print(f"+{int(reward*10)} points!")
            elif reward < 0:
                print(f"{int(reward*10)} points")
        else:
            print("Invalid input, use w/a/s/d")

# ============ 6. Main Program ============
if __name__ == "__main__":
    print("Select Mode:")
    print("1. Train Q-learning AI")
    print("2. Play Game (Manual Control)")
    print("3. AI Demo Game (Watch AI Play)")

    choice = input("Enter 1-3: ")

    if choice == "1":
        agent = train_q_learning(episodes=700)
        print("Training complete! Model saved as 'trained_agent.pkl'")
        import numpy as np
        import matplotlib.pyplot as plt

        rewards = agent.training_history['rewards']
        plt.plot(rewards, label="Episode Reward")

        # 滑动平均
        window = 50
        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')
        plt.plot(range(window-1, len(rewards)), moving_avg, label=f"Moving Avg ({window})")

        plt.xlabel("Episode")
        plt.ylabel("Reward")
        plt.title("Q-learning Training Progress")
        plt.legend()
        plt.show()

    elif choice == "2":
        play_game()
    elif choice == "3":
        print("\n" + "="*60)
        print("Note: Need to train AI first (Mode 1) to generate 'trained_agent.pkl'")
        print("="*60)
        confirm = input("Continue? (y/n): ").lower()
        if confirm == 'y':
            ai_demo_game(episodes=2, step_delay=0.3)
    else:
        print("Invalid selection")